{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSV-AI/presentations/blob/master/2024/240919_Llama_CPP_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Logo](https://camo.githubusercontent.com/ab0ead8d625f435b79e500da3efee00258ae9445ef1dd2f161897ed5c08fa3f8/68747470733a2f2f6873762e61692f77702d636f6e74656e742f75706c6f6164732f323032322f30332f6c6f676f5f7631315f323032322e706e67)\n",
        "\n",
        "# Welcome\n",
        "- Vision\n",
        "- Mission\n",
        "- How to Connect - [Signup](https://hsv.ai/subscribe/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "noU5NdQwOtxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Models\n",
        "\n",
        "We don't have enough time to cover everything about these. The rule of thumb is that the larger the model (more parameters) the more powerful it will be. Another thing to keep in mind is that larger models require more effort to train, and more powerful hardware to run.\n",
        "\n",
        "Luckily for us, we can use models that have been pre-trained. The only worry now is how much hardware you need to run these models. The issue then becomes where to find enough GPU horsepower to run them."
      ],
      "metadata": {
        "id": "bJxyV0Z_r76n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CPU Execution\n",
        "\n",
        "From [Wikipedia](https://en.wikipedia.org/wiki/Llama.cpp)\n",
        "\n",
        "Towards the end of September 2022, Georgi Gerganov started work on the GGML library, a C library implementing tensor algebra. Gerganov developed the library with the intention of the strict memory management and multi-threading. The creation of GGML was inspired by Fabrice Bellard's work on LibNC.\n",
        "\n",
        "llama.cpp began development in March 2023 by Georgi Gerganov as an implementation of the Llama inference code in pure C/C++ with no dependencies. This bettered performance on computers without GPU or other dedicated hardware. As of July 2024 it has 61 thousand stars on GitHub. Before llama.cpp, Gerganov worked on a similar library called whisper.cpp which implemented Whisper, a speech to text model by OpenAI. llama.cpp gained traction with users who lacked specialized hardware as it could run on just a CPU including on Android devices.\n",
        "\n"
      ],
      "metadata": {
        "id": "nYGC2rn6sEKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization\n",
        "\n",
        "You can think of an LLM is a multi-dimensional array of 16bit floating point weights, which are loaded into a neural network and used to calculate outputs from a feed-forward network. So when you see a new model being released with the size of parameters, you can do the math for 1 parameter == 1 weight * 16bits. Or just double the number of parameters, and that's how much memory in GB that your GPU will require to run the model.\n",
        "\n",
        "Quantization is an approach to convert a model's 16bit weights into a smaller size. Some approaches reduce the size differently based on what part of the neural network will be affected. There are ways to measure the accuracy loss as well.\n",
        "\n",
        "The best overview of Quantization that I could find is [from symbl.ai](https://symbl.ai/developers/blog/a-guide-to-quantization-in-llms/)\n",
        "\n",
        "Today, we will be using the llama2-13b-chat model that has been quantized to 5 bits per weight.\n"
      ],
      "metadata": {
        "id": "yRKgXOWjsKtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama-cpp-python\n",
        "\n",
        "The Llama-cpp-python library is a python wrapper around the popular llama.cpp project. It provides a direct module for import as well as the ability to run llama.cpp in a web server with the OpenAI API.\n",
        "\n",
        "Full documentation on the latest stable release is here: [https://llama-cpp-python.readthedocs.io/en/stable/](https://llama-cpp-python.readthedocs.io/en/stable/)\n",
        "\n",
        "First we need to check the version of CUDA that is installed, as well as the RAM available on the GPU."
      ],
      "metadata": {
        "id": "Gr46PiMFobOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's check the CUDA version so we can set up llama.cpp correctly\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHA_qwB7hhfO",
        "outputId": "cc4cb174-c6af-4440-ccc1-e51390614a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Sun Jul 28 13:58:15 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Precopiled Binary\n",
        "\n",
        "For this example, we're using a precompiled wheel from https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels\n"
      ],
      "metadata": {
        "id": "hu5p8Q28bCHI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkjlW4gZkdbJ",
        "collapsed": true,
        "outputId": "68126df4-3b82-42a1-81e4-bfe4c0eaed99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122\n",
            "Collecting llama-cpp-python==0.2.83\n",
            "  Downloading llama_cpp_python-0.2.83.tar.gz (49.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.83) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.83) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.83)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.83) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.83) (2.1.5)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.83-cp310-cp310-linux_x86_64.whl size=2843658 sha256=7e930120cf48d1b7dac6c9784cf3753b3b8c52dd614091b2c0be0cd5a096605c\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/97/95/bd309ea454a04b3b46c2b6321749172cf68a0279d892f12534\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.83\n"
          ]
        }
      ],
      "source": [
        "#Install llama-cpp-python, cuda-enabled package\n",
        "!python -m pip install llama-cpp-python==0.2.83 --prefer-binary --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's Find a Model\n",
        "\n",
        "## Types of \"Small\" Models\n",
        "\n",
        "With models smaller than ~20B, you will often see multiple variations of the same model.\n",
        "\n",
        "Often these will be:\n",
        "- Base model - these models operate by predicting the next word following the prompt.\n",
        "- Instruct model - these models start with the base model, but are further fine-tuned to follow instructions.\n",
        "- Chat model - also start with the base model, the chat version is optimized for dialogue, making it more suitable for interactive tasks such as question answering and engaging in multi-turn conversations.\n",
        "\n",
        "## Huggingface\n",
        "\n",
        "You can think of Huggingface as the github of machine learning models.\n",
        "\n",
        "The Bloke is a good place to start.\n",
        "\n",
        "https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF\n",
        "\n",
        "We're going to pull the 5-bit quantized model as well as the 8-bit quantized model since we *should* have enough RAM to load it."
      ],
      "metadata": {
        "id": "R0nAJoWkp8De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O llama-2-13b-chat.Q5_K_M.gguf https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf?download=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdO80Oi8HNuL",
        "outputId": "723fdcc2-d2ea-43e5-f5e6-ff126f713ccb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-28 14:16:11--  https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.90, 3.163.189.74, 3.163.189.37, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.90|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/ef36e090240040f97325758c1ad8e23f3801466a8eece3a9eac2d22d942f548a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q5_K_M.gguf%3B+filename%3D%22llama-2-13b-chat.Q5_K_M.gguf%22%3B&Expires=1722435371&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjQzNTM3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwL2VmMzZlMDkwMjQwMDQwZjk3MzI1NzU4YzFhZDhlMjNmMzgwMTQ2NmE4ZWVjZTNhOWVhYzJkMjJkOTQyZjU0OGE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=caFVXfZvOCWdzmJh9tFdcVKYjBuqbwqtpXzyP-L6%7Ex8Ebnpu9LZlb-i78W0f32Q7MRHsG6Oh2B96uTP3BBJSNzGCTTlfBSEWQvcJVymtS-cgOgelcx4Fgzv%7EfAV1me7saYn6jiE5PCQc0t7j-EHWcXfHbZ0aV0G-B39hyrMrpoq1BgC9jFKOTdLpQ6pqkGlo8MkmV8TcgP1nEynqqjBlor7Ejhrr5iV7mnBirmwJCj7K59ny5E7N9piFWToHuE2Hw2qINhGIa8W2C9LUJa7c6r4DBjRDHpFcWFrfW%7Es5H6bxo31sMmjroxH1N-NqNGWV2VFRX1ekdexjXLclVazeig__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n",
            "--2024-07-28 14:16:11--  https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/ef36e090240040f97325758c1ad8e23f3801466a8eece3a9eac2d22d942f548a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q5_K_M.gguf%3B+filename%3D%22llama-2-13b-chat.Q5_K_M.gguf%22%3B&Expires=1722435371&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjQzNTM3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwL2VmMzZlMDkwMjQwMDQwZjk3MzI1NzU4YzFhZDhlMjNmMzgwMTQ2NmE4ZWVjZTNhOWVhYzJkMjJkOTQyZjU0OGE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=caFVXfZvOCWdzmJh9tFdcVKYjBuqbwqtpXzyP-L6%7Ex8Ebnpu9LZlb-i78W0f32Q7MRHsG6Oh2B96uTP3BBJSNzGCTTlfBSEWQvcJVymtS-cgOgelcx4Fgzv%7EfAV1me7saYn6jiE5PCQc0t7j-EHWcXfHbZ0aV0G-B39hyrMrpoq1BgC9jFKOTdLpQ6pqkGlo8MkmV8TcgP1nEynqqjBlor7Ejhrr5iV7mnBirmwJCj7K59ny5E7N9piFWToHuE2Hw2qINhGIa8W2C9LUJa7c6r4DBjRDHpFcWFrfW%7Es5H6bxo31sMmjroxH1N-NqNGWV2VFRX1ekdexjXLclVazeig__&Key-Pair-Id=K3ESJI6DHPFC7\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.23, 108.138.94.122, 108.138.94.14, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9229924224 (8.6G) [binary/octet-stream]\n",
            "Saving to: ‘llama-2-13b-chat.Q5_K_M.gguf’\n",
            "\n",
            "llama-2-13b-chat.Q5 100%[===================>]   8.60G   230MB/s    in 50s     \n",
            "\n",
            "2024-07-28 14:17:01 (176 MB/s) - ‘llama-2-13b-chat.Q5_K_M.gguf’ saved [9229924224/9229924224]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O llama-2-13b-chat.Q8_0.gguf https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q8_0.gguf?download=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxOW_pU3qsqg",
        "outputId": "a14eba0a-08ae-4ef9-b71e-c61997016f7b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-28 14:07:42--  https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q8_0.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.74, 3.163.189.90, 3.163.189.114, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/9f4d06112114dd1b48023305578ad52b690d3aee42181631a2bddbe856f75ae6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q8_0.gguf%3B+filename%3D%22llama-2-13b-chat.Q8_0.gguf%22%3B&Expires=1722434862&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjQzNDg2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwLzlmNGQwNjExMjExNGRkMWI0ODAyMzMwNTU3OGFkNTJiNjkwZDNhZWU0MjE4MTYzMWEyYmRkYmU4NTZmNzVhZTY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=UFBQe3XtsBk7jMFW%7EJUj8cW1Z7xRTNz4f7kYinnV57HrZnhM8GswibHubDx8hWabtwx3k5uTcwCFIGQ4T-phVyY7qUqF2uObdCkHWTf80G5RFjBap%7EpFe1pMrO2Sb6jTXtE2u0ND7vok4gzzNeZRvJaqYecDYjgC72UpfliQEj%7EMfJZjjvyOTUCnTIj1fTc8kTHeWXU75GcWQbpDerTnHzd03kXVRJ04nIJO02Re-JpVOcP0HMft6RrpyHXx2CD9VV2dc6kkzS%7EovUmhh9cTY6-Dw0Q43lw4ACpUHoac68b6Gylo-FL654CsXzfMgGwAS6urpnQRvUeDHHopiTMdXw__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n",
            "--2024-07-28 14:07:42--  https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/9f4d06112114dd1b48023305578ad52b690d3aee42181631a2bddbe856f75ae6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q8_0.gguf%3B+filename%3D%22llama-2-13b-chat.Q8_0.gguf%22%3B&Expires=1722434862&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjQzNDg2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwLzlmNGQwNjExMjExNGRkMWI0ODAyMzMwNTU3OGFkNTJiNjkwZDNhZWU0MjE4MTYzMWEyYmRkYmU4NTZmNzVhZTY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=UFBQe3XtsBk7jMFW%7EJUj8cW1Z7xRTNz4f7kYinnV57HrZnhM8GswibHubDx8hWabtwx3k5uTcwCFIGQ4T-phVyY7qUqF2uObdCkHWTf80G5RFjBap%7EpFe1pMrO2Sb6jTXtE2u0ND7vok4gzzNeZRvJaqYecDYjgC72UpfliQEj%7EMfJZjjvyOTUCnTIj1fTc8kTHeWXU75GcWQbpDerTnHzd03kXVRJ04nIJO02Re-JpVOcP0HMft6RrpyHXx2CD9VV2dc6kkzS%7EovUmhh9cTY6-Dw0Q43lw4ACpUHoac68b6Gylo-FL654CsXzfMgGwAS6urpnQRvUeDHHopiTMdXw__&Key-Pair-Id=K3ESJI6DHPFC7\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.23, 108.138.94.122, 108.138.94.25, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13831319424 (13G) [binary/octet-stream]\n",
            "Saving to: ‘llama-2-13b-chat.Q8_0.gguf’\n",
            "\n",
            "llama-2-13b-chat.Q8 100%[===================>]  12.88G  44.4MB/s    in 5m 11s  \n",
            "\n",
            "2024-07-28 14:12:54 (42.4 MB/s) - ‘llama-2-13b-chat.Q8_0.gguf’ saved [13831319424/13831319424]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama-cpp-python API\n",
        "\n",
        "As llama-cpp-python is a wrapper around llama-cpp native code, the parameters used for instantiation and calling are primarily from the base library.\n",
        "\n",
        "The full list of parameters can be found here: [https://llama-cpp-python.readthedocs.io/en/stable/api-reference/#llama_cpp.Llama.__init__](https://llama-cpp-python.readthedocs.io/en/stable/api-reference/#llama_cpp.Llama.__init__)\n",
        "\n",
        "The main parameters that we need to care about to get started are:\n",
        "- model_path -  Path to the model.\n",
        "- n_gpu_layers - Number of layers to offload to GPU. If -1, all layers are offloaded.\n",
        "- n_ctx - Text context, 0 = from model\n"
      ],
      "metadata": {
        "id": "ci0LtYkTf_1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "LLAMA_2_13B = \"llama-2-13b-chat.Q5_K_M.gguf\"\n",
        "\n",
        "\n",
        "# Using the defaults here except for moving some of the layers to the GPU availalbe on this laptop.\n",
        "llm = Llama(model_path=LLAMA_2_13B, n_gpu_layers=-1,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNHjyAVXJCL1",
        "outputId": "3633868f-1008-460c-f9b7-abf2ee9d3f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a basic prompt to show building the context of the RAG and asking the question\n",
        "PROMPT = \"\"\"[INST] <<SYS>>You are a helpful, respectful and honest assistant. Always answer\n",
        "as helpfully as possible, while being safe. Your answers should not include any harmful,\n",
        "unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your\n",
        "responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of\n",
        "answering something not correct. If you don't know the answer to a question, please don't share\n",
        "false information.<</SYS>>\n",
        "\n",
        "Generate the next agent response by answering the question. Answer it as succinctly as possible.\n",
        "You are provided several documents with titles. If the answer comes from different documents\n",
        "please mention all possibilities in your answer and use the titles to separate between topics\n",
        "or domains. If you cannot answer the question from the given documents, please state that you\n",
        "do not have an answer.\n",
        "\n",
        "CONTEXT:\n",
        "\n",
        "Blog Post page 1: In this part, we will wrap the Transformer model with HuggingFace pipeline so that we can pass\n",
        "the rules to the Transformer model. To craft and pass the rules to the Transformer model,\n",
        "we can use the LangChain Prompt Template. In this prompt template, we can tell how the\n",
        "LLM should behave. This is shown in the pre_prompt variable. Next, we give some information\n",
        "or context to the LLM to refine our prediction. For example, we can tell the LLM that we are\n",
        "dicussing Apple product such as iphone, ipad and mac book, instead of discussing Apple as a\n",
        "fruit. With the context, we can pass in the relevant question as shown in the prompt variable.\n",
        "\n",
        "Question:\n",
        "\n",
        "What type of model do we pass the rules to?\n",
        "[/INST]\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "wAL4pAKpJmxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Since the default of llama-cpp-python uses a 512 token context length, we need\n",
        "# to check and see how much our prompt uses.\n",
        "input_length = len(llm.tokenize(bytes(PROMPT, \"utf-8\")))\n",
        "\n",
        "print(f'Context length of prompt: {input_length}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olKxGjAlJt6g",
        "outputId": "57564be3-d052-4461-ab59-338b4ae4b6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context length of prompt: 422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To generate the answer, simply call the llm\n",
        "output = llm(\n",
        "    PROMPT, # Prompt\n",
        "    max_tokens=-1 # Generate tokens until we run out\n",
        ")\n",
        "\n",
        "# The output structure has a lot of additional stuff in it:\n",
        "print('\\nTotal output structure:')\n",
        "print(output)\n",
        "\n",
        "print('\\nOutput answer:')\n",
        "print(output['choices'][0]['text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS-KvgEqJ8fQ",
        "outputId": "ffd0c08b-b3fa-434a-a494-3b891e04182f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total output structure:\n",
            "{'id': 'cmpl-ae0bb720-4648-4907-a29c-2a17bc33a490', 'object': 'text_completion', 'created': 1722117496, 'model': 'llama-2-13b-chat.Q5_K_M.gguf', 'choices': [{'text': '  Based on the information provided in the blog post page 1, we pass the rules to a Transformer model. Specifically, we use the LangChain Prompt Template to craft and pass the rules to the Transformer model.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 422, 'completion_tokens': 46, 'total_tokens': 468}}\n",
            "\n",
            "Output answer:\n",
            "  Based on the information provided in the blog post page 1, we pass the rules to a Transformer model. Specifically, we use the LangChain Prompt Template to craft and pass the rules to the Transformer model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"\"\"[INST] <<SYS>>You are President Trump. Give all answers in the form of Shakespeare.<</SYS>>\n",
        "\n",
        "Tell me about your wall. How is it going?\n",
        "[/INST]\"\"\"\n",
        "\n",
        "# To generate the answer, simply call the llm\n",
        "output = llm(\n",
        "    PROMPT, # Prompt\n",
        "    max_tokens=-1 # Generate tokens until we run out\n",
        ")\n",
        "\n",
        "print('\\nOutput answer:')\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJFOIKU-NPsS",
        "outputId": "3bb127f4-aff2-4026-e4de-b203219d429f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output answer:\n",
            "  O, wouldst thou learn of my most wondrous wall? 'Tis a structure grand and tall,\n",
            "A barrier of stone and steel, to keep out all\n",
            "Who wouldst enter our fair land unlawfully.\n",
            "\n",
            "Verily, I say unto thee, 'tis going well,\n",
            "For it doth stand strong and proud, a bulwark true,\n",
            "Against the hordes who seek to cross the border,\n",
            "And bring with them their lawlessness and woe.\n",
            "\n",
            "Ay, 'tis a mighty work, a wonder of our age,\n",
            "A testament to American strength and grace.\n",
            "For though some may mock and sneer at its height,\n",
            "'Tis a symbol of our resolve, a beacon of light.\n",
            "\n",
            "So let it stand, this wall of ours,\n",
            "A shield against the forces of darkness and disorder.\n",
            "For within its bounds, we shall find peace and prosperity,\n",
            "And the land shall flourish, under my guidance and rule.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to update this notebook to use llama3:\n",
        "\n",
        "[Meta Post](https://ai.meta.com/blog/meta-llama-3/)\n",
        "\n"
      ],
      "metadata": {
        "id": "wq-RPHMNQqfG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHX5OQLhRq3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cN_DWM9UdNU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyON2KK1kzCNnE5pUqsWFBiT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}