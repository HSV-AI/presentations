{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSV-AI/presentations/blob/master/2024/240919_Llama_CPP_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Logo](https://camo.githubusercontent.com/455da7518417340e112a473e3bdd91dae3dc8fda296d247ad3f3bc95cced8738/68747470733a2f2f6873762e61692f77702d636f6e74656e742f75706c6f6164732f323032322f30332f6c6f676f5f7631315f323032322e706e67)\n",
        "\n",
        "# Welcome\n",
        "- Vision\n",
        "- Mission\n",
        "- How to Connect - [Signup](https://hsv.ai/subscribe/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "noU5NdQwOtxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Models\n",
        "\n",
        "We don't have enough time to cover everything about these. The rule of thumb is that the larger the model (more parameters) the more powerful it will be. Another thing to keep in mind is that larger models require more effort to train, and more powerful hardware to run.\n",
        "\n",
        "Luckily for us, we can use models that have been pre-trained. The only worry now is how much hardware you need to run these models. The issue then becomes where to find enough GPU horsepower to run them."
      ],
      "metadata": {
        "id": "bJxyV0Z_r76n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CPU Execution\n",
        "\n",
        "From [Wikipedia](https://en.wikipedia.org/wiki/Llama.cpp)\n",
        "\n",
        "Towards the end of September 2022, Georgi Gerganov started work on the GGML library, a C library implementing tensor algebra. Gerganov developed the library with the intention of the strict memory management and multi-threading. The creation of GGML was inspired by Fabrice Bellard's work on LibNC.\n",
        "\n",
        "llama.cpp began development in March 2023 by Georgi Gerganov as an implementation of the Llama inference code in pure C/C++ with no dependencies. This bettered performance on computers without GPU or other dedicated hardware. As of July 2024 it has 61 thousand stars on GitHub. Before llama.cpp, Gerganov worked on a similar library called whisper.cpp which implemented Whisper, a speech to text model by OpenAI. llama.cpp gained traction with users who lacked specialized hardware as it could run on just a CPU including on Android devices.\n",
        "\n"
      ],
      "metadata": {
        "id": "nYGC2rn6sEKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization\n",
        "\n",
        "You can think of an LLM is a multi-dimensional array of 16bit floating point weights, which are loaded into a neural network and used to calculate outputs from a feed-forward network. So when you see a new model being released with the size of parameters, you can do the math for 1 parameter == 1 weight * 16bits. Or just double the number of parameters, and that's how much memory in GB that your GPU will require to run the model.\n",
        "\n",
        "Quantization is an approach to convert a model's 16bit weights into a smaller size. Some approaches reduce the size differently based on what part of the neural network will be affected. There are ways to measure the accuracy loss as well.\n",
        "\n",
        "The best overview of Quantization that I could find is [from symbl.ai](https://symbl.ai/developers/blog/a-guide-to-quantization-in-llms/)\n",
        "\n",
        "Today, we will be using the llama2-13b-chat model that has been quantized to 5 bits per weight.\n"
      ],
      "metadata": {
        "id": "yRKgXOWjsKtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama-cpp-python\n",
        "\n",
        "The Llama-cpp-python library is a python wrapper around the popular llama.cpp project. It provides a direct module for import as well as the ability to run llama.cpp in a web server with the OpenAI API.\n",
        "\n",
        "Full documentation on the latest stable release is here: [https://llama-cpp-python.readthedocs.io/en/stable/](https://llama-cpp-python.readthedocs.io/en/stable/)\n",
        "\n",
        "First we need to check the version of CUDA that is installed, as well as the RAM available on the GPU."
      ],
      "metadata": {
        "id": "Gr46PiMFobOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's check the CUDA version so we can set up llama.cpp correctly\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHA_qwB7hhfO",
        "outputId": "adf8dbbd-f049-4197-9824-ebdd10860beb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Thu Sep 19 02:25:43 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Precompiled Binary\n",
        "\n",
        "For this example, we're using a precompiled wheel from https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels\n"
      ],
      "metadata": {
        "id": "hu5p8Q28bCHI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkjlW4gZkdbJ",
        "collapsed": true,
        "outputId": "85f32f1a-b5a9-4f64-9f0f-ac7f5c836fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122\n",
            "Collecting llama-cpp-python==0.2.83\n",
            "  Downloading llama_cpp_python-0.2.83.tar.gz (49.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.83) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.83) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.83)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.83) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.83) (2.1.5)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.83-cp310-cp310-linux_x86_64.whl size=2860110 sha256=e17d281d24dd222d0609277b4cf77cc7cba60c7a1e2a3075b811f83e00faf6e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/97/95/bd309ea454a04b3b46c2b6321749172cf68a0279d892f12534\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.83\n"
          ]
        }
      ],
      "source": [
        "#Install llama-cpp-python, cuda-enabled package\n",
        "!python -m pip install llama-cpp-python==0.2.83 --prefer-binary --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install llama-cpp-python with CUBLAS, compatible to CUDA 12.2 which is the CUDA driver build above\n",
        "!set LLAMA_CUBLAS=1\n",
        "!set CMAKE_ARGS=-DLLAMA_CUBLAS=on\n",
        "!set FORCE_CMAKE=1\n",
        "\n",
        "#Install llama-cpp-python, cuda-enabled package\n",
        "!python -m pip install llama-cpp-python==0.2.7 --prefer-binary --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122\n",
        "\n",
        "#Install pytorch-related, cuda-enabled package\n",
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Apyiw7XQckT",
        "outputId": "6ea7997a-d5ba-4c35-9e26-4ab4ec64da15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122\n",
            "Requirement already satisfied: llama-cpp-python==0.2.7 in /usr/local/lib/python3.10/dist-packages (0.2.7+cu122)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.7) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.7) (1.26.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.7) (5.6.3)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (781.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.18.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (10.4.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0+cu121\n",
            "    Uninstalling torch-2.4.0+cu121:\n",
            "      Successfully uninstalled torch-2.4.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.0+cu121\n",
            "    Uninstalling torchvision-0.19.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.19.0+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.4.0+cu121\n",
            "    Uninstalling torchaudio-2.4.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.4.0+cu121\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 torch-2.3.0+cu121 torchaudio-2.3.0+cu121 torchvision-0.18.0+cu121 triton-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Working for GPU\n",
        "%%writefile gpu_requirements.txt\n",
        "annotated-types==0.7.0\n",
        "anyio==4.4.0\n",
        "certifi==2022.12.7\n",
        "charset-normalizer==2.1.1\n",
        "click==8.1.7\n",
        "colorama==0.4.6\n",
        "diskcache==5.6.3\n",
        "dnspython==2.6.1\n",
        "email_validator==2.1.1\n",
        "exceptiongroup==1.2.1\n",
        "filelock==3.13.1\n",
        "fsspec==2024.6.0\n",
        "h11==0.14.0\n",
        "httpcore==1.0.5\n",
        "httptools==0.6.1\n",
        "httpx==0.27.0\n",
        "huggingface-hub==0.23.3\n",
        "idna==3.4\n",
        "Jinja2==3.1.4\n",
        "llama_cpp_python==0.2.7+cu122\n",
        "markdown-it-py==3.0.0\n",
        "MarkupSafe==2.1.5\n",
        "mdurl==0.1.2\n",
        "mpmath==1.3.0\n",
        "networkx==3.2.1\n",
        "numpy==1.26.4\n",
        "orjson==3.10.3\n",
        "packaging==24.0\n",
        "pillow==10.2.0\n",
        "pydantic==2.7.3\n",
        "pydantic_core==2.18.4\n",
        "Pygments==2.18.0\n",
        "python-dotenv==1.0.1\n",
        "python-multipart==0.0.9\n",
        "PyYAML==6.0.1\n",
        "requests==2.28.1\n",
        "rich==13.7.1\n",
        "shellingham==1.5.4\n",
        "sniffio==1.3.1\n",
        "starlette==0.37.2\n",
        "sympy==1.12\n",
        "torch==2.3.0+cu121\n",
        "torchaudio==2.3.0+cu121\n",
        "torchvision==0.18.0+cu121\n",
        "tqdm==4.66.4\n",
        "typer==0.12.3\n",
        "typing_extensions==4.12.1\n",
        "ujson==5.10.0\n",
        "watchfiles==0.22.0"
      ],
      "metadata": {
        "id": "5aB01vYIS45w",
        "outputId": "70487ec9-b839-461a-ede5-6ecad9fbdba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gpu_requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r gpu_requirements.txt #it's normal to see incompatiblity errors; the most important packages have been installed correctly\n"
      ],
      "metadata": {
        "id": "QUnaXEJaTDfr",
        "outputId": "7dcc037c-677f-4478-b31c-6ab78e532fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 1)) (0.7.0)\n",
            "Collecting anyio==4.4.0 (from -r gpu_requirements.txt (line 2))\n",
            "  Downloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting certifi==2022.12.7 (from -r gpu_requirements.txt (line 3))\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting charset-normalizer==2.1.1 (from -r gpu_requirements.txt (line 4))\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 5)) (8.1.7)\n",
            "Collecting colorama==0.4.6 (from -r gpu_requirements.txt (line 6))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 7)) (5.6.3)\n",
            "Collecting dnspython==2.6.1 (from -r gpu_requirements.txt (line 8))\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting email_validator==2.1.1 (from -r gpu_requirements.txt (line 9))\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting exceptiongroup==1.2.1 (from -r gpu_requirements.txt (line 10))\n",
            "  Downloading exceptiongroup-1.2.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting filelock==3.13.1 (from -r gpu_requirements.txt (line 11))\n",
            "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting fsspec==2024.6.0 (from -r gpu_requirements.txt (line 12))\n",
            "  Downloading fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting h11==0.14.0 (from -r gpu_requirements.txt (line 13))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting httpcore==1.0.5 (from -r gpu_requirements.txt (line 14))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting httptools==0.6.1 (from -r gpu_requirements.txt (line 15))\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting httpx==0.27.0 (from -r gpu_requirements.txt (line 16))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting huggingface-hub==0.23.3 (from -r gpu_requirements.txt (line 17))\n",
            "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting idna==3.4 (from -r gpu_requirements.txt (line 18))\n",
            "  Downloading idna-3.4-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: Jinja2==3.1.4 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 19)) (3.1.4)\n",
            "Requirement already satisfied: llama_cpp_python==0.2.7+cu122 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 20)) (0.2.7+cu122)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 21)) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe==2.1.5 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 22)) (2.1.5)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 23)) (0.1.2)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 24)) (1.3.0)\n",
            "Collecting networkx==3.2.1 (from -r gpu_requirements.txt (line 25))\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 26)) (1.26.4)\n",
            "Collecting orjson==3.10.3 (from -r gpu_requirements.txt (line 27))\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging==24.0 (from -r gpu_requirements.txt (line 28))\n",
            "  Downloading packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pillow==10.2.0 (from -r gpu_requirements.txt (line 29))\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting pydantic==2.7.3 (from -r gpu_requirements.txt (line 30))\n",
            "  Downloading pydantic-2.7.3-py3-none-any.whl.metadata (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic_core==2.18.4 (from -r gpu_requirements.txt (line 31))\n",
            "  Downloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: Pygments==2.18.0 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 32)) (2.18.0)\n",
            "Collecting python-dotenv==1.0.1 (from -r gpu_requirements.txt (line 33))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting python-multipart==0.0.9 (from -r gpu_requirements.txt (line 34))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting PyYAML==6.0.1 (from -r gpu_requirements.txt (line 35))\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting requests==2.28.1 (from -r gpu_requirements.txt (line 36))\n",
            "  Downloading requests-2.28.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting rich==13.7.1 (from -r gpu_requirements.txt (line 37))\n",
            "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 38)) (1.5.4)\n",
            "Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 39)) (1.3.1)\n",
            "Collecting starlette==0.37.2 (from -r gpu_requirements.txt (line 40))\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting sympy==1.12 (from -r gpu_requirements.txt (line 41))\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: torch==2.3.0+cu121 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 42)) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchaudio==2.3.0+cu121 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 43)) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision==0.18.0+cu121 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 44)) (0.18.0+cu121)\n",
            "Collecting tqdm==4.66.4 (from -r gpu_requirements.txt (line 45))\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer==0.12.3 (from -r gpu_requirements.txt (line 46))\n",
            "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting typing_extensions==4.12.1 (from -r gpu_requirements.txt (line 47))\n",
            "  Downloading typing_extensions-4.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting ujson==5.10.0 (from -r gpu_requirements.txt (line 48))\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting watchfiles==0.22.0 (from -r gpu_requirements.txt (line 49))\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1 (from requests==2.28.1->-r gpu_requirements.txt (line 36))\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0+cu121->-r gpu_requirements.txt (line 42)) (12.1.105)\n",
            "Downloading anyio-4.4.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
            "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.7.3-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.1-py3-none-any.whl (37 kB)\n",
            "Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, ujson, typing_extensions, tqdm, sympy, PyYAML, python-multipart, python-dotenv, pillow, packaging, orjson, networkx, idna, httptools, h11, fsspec, filelock, exceptiongroup, dnspython, colorama, charset-normalizer, certifi, rich, requests, pydantic_core, httpcore, email_validator, anyio, watchfiles, typer, starlette, pydantic, huggingface-hub, httpx\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.2\n",
            "    Uninstalling sympy-1.13.2:\n",
            "      Successfully uninstalled sympy-1.13.2\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 10.4.0\n",
            "    Uninstalling pillow-10.4.0:\n",
            "      Successfully uninstalled pillow-10.4.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.16.0\n",
            "    Uninstalling filelock-3.16.0:\n",
            "      Successfully uninstalled filelock-3.16.0\n",
            "  Attempting uninstall: exceptiongroup\n",
            "    Found existing installation: exceptiongroup 1.2.2\n",
            "    Uninstalling exceptiongroup-1.2.2:\n",
            "      Successfully uninstalled exceptiongroup-1.2.2\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2024.8.30\n",
            "    Uninstalling certifi-2024.8.30:\n",
            "      Successfully uninstalled certifi-2024.8.30\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.8.1\n",
            "    Uninstalling rich-13.8.1:\n",
            "      Successfully uninstalled rich-13.8.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pydantic_core\n",
            "    Found existing installation: pydantic_core 2.23.3\n",
            "    Uninstalling pydantic_core-2.23.3:\n",
            "      Successfully uninstalled pydantic_core-2.23.3\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.12.5\n",
            "    Uninstalling typer-0.12.5:\n",
            "      Successfully uninstalled typer-0.12.5\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.9.1\n",
            "    Uninstalling pydantic-2.9.1:\n",
            "      Successfully uninstalled pydantic-2.9.1\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.6.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.28.1 which is incompatible.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.4.0 which is incompatible.\n",
            "kaggle 1.6.17 requires certifi>=2023.7.22, but you have certifi 2022.12.7 which is incompatible.\n",
            "pytensor 2.25.4 requires filelock>=3.15, but you have filelock 3.13.1 which is incompatible.\n",
            "yfinance 0.2.43 requires requests>=2.31, but you have requests 2.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.1 anyio-4.4.0 certifi-2022.12.7 charset-normalizer-2.1.1 colorama-0.4.6 dnspython-2.6.1 email_validator-2.1.1 exceptiongroup-1.2.1 filelock-3.13.1 fsspec-2024.6.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface-hub-0.23.3 idna-3.4 networkx-3.2.1 orjson-3.10.3 packaging-24.0 pillow-10.2.0 pydantic-2.7.3 pydantic_core-2.18.4 python-dotenv-1.0.1 python-multipart-0.0.9 requests-2.28.1 rich-13.7.1 starlette-0.37.2 sympy-1.12 tqdm-4.66.4 typer-0.12.3 typing_extensions-4.12.1 ujson-5.10.0 urllib3-1.26.20 watchfiles-0.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi"
                ]
              },
              "id": "bf0fe5355dbb48ed8f6f088c654f5dda"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's Find a Model\n",
        "\n",
        "## Types of \"Small\" Models\n",
        "\n",
        "With models smaller than ~20B, you will often see multiple variations of the same model.\n",
        "\n",
        "Often these will be:\n",
        "- Base model - these models operate by predicting the next word following the prompt.\n",
        "- Instruct model - these models start with the base model, but are further fine-tuned to follow instructions.\n",
        "- Chat model - also start with the base model, the chat version is optimized for dialogue, making it more suitable for interactive tasks such as question answering and engaging in multi-turn conversations.\n",
        "\n",
        "## Huggingface\n",
        "\n",
        "You can think of Huggingface as the github of machine learning models.\n",
        "\n",
        "The Bloke is a good place to start.\n",
        "\n",
        "https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF\n",
        "\n",
        "We're going to pull the 5-bit quantized model as well as the 8-bit quantized model since we *should* have enough RAM to load it."
      ],
      "metadata": {
        "id": "R0nAJoWkp8De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O llama-2-13b-chat.Q5_K_M.gguf https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf?download=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdO80Oi8HNuL",
        "outputId": "723fdcc2-d2ea-43e5-f5e6-ff126f713ccb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-28 14:16:11--  https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.90, 3.163.189.74, 3.163.189.37, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.90|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/ef36e090240040f97325758c1ad8e23f3801466a8eece3a9eac2d22d942f548a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q5_K_M.gguf%3B+filename%3D%22llama-2-13b-chat.Q5_K_M.gguf%22%3B&Expires=1722435371&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjQzNTM3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwL2VmMzZlMDkwMjQwMDQwZjk3MzI1NzU4YzFhZDhlMjNmMzgwMTQ2NmE4ZWVjZTNhOWVhYzJkMjJkOTQyZjU0OGE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=caFVXfZvOCWdzmJh9tFdcVKYjBuqbwqtpXzyP-L6%7Ex8Ebnpu9LZlb-i78W0f32Q7MRHsG6Oh2B96uTP3BBJSNzGCTTlfBSEWQvcJVymtS-cgOgelcx4Fgzv%7EfAV1me7saYn6jiE5PCQc0t7j-EHWcXfHbZ0aV0G-B39hyrMrpoq1BgC9jFKOTdLpQ6pqkGlo8MkmV8TcgP1nEynqqjBlor7Ejhrr5iV7mnBirmwJCj7K59ny5E7N9piFWToHuE2Hw2qINhGIa8W2C9LUJa7c6r4DBjRDHpFcWFrfW%7Es5H6bxo31sMmjroxH1N-NqNGWV2VFRX1ekdexjXLclVazeig__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n",
            "--2024-07-28 14:16:11--  https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/ef36e090240040f97325758c1ad8e23f3801466a8eece3a9eac2d22d942f548a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q5_K_M.gguf%3B+filename%3D%22llama-2-13b-chat.Q5_K_M.gguf%22%3B&Expires=1722435371&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjQzNTM3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwL2VmMzZlMDkwMjQwMDQwZjk3MzI1NzU4YzFhZDhlMjNmMzgwMTQ2NmE4ZWVjZTNhOWVhYzJkMjJkOTQyZjU0OGE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=caFVXfZvOCWdzmJh9tFdcVKYjBuqbwqtpXzyP-L6%7Ex8Ebnpu9LZlb-i78W0f32Q7MRHsG6Oh2B96uTP3BBJSNzGCTTlfBSEWQvcJVymtS-cgOgelcx4Fgzv%7EfAV1me7saYn6jiE5PCQc0t7j-EHWcXfHbZ0aV0G-B39hyrMrpoq1BgC9jFKOTdLpQ6pqkGlo8MkmV8TcgP1nEynqqjBlor7Ejhrr5iV7mnBirmwJCj7K59ny5E7N9piFWToHuE2Hw2qINhGIa8W2C9LUJa7c6r4DBjRDHpFcWFrfW%7Es5H6bxo31sMmjroxH1N-NqNGWV2VFRX1ekdexjXLclVazeig__&Key-Pair-Id=K3ESJI6DHPFC7\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.23, 108.138.94.122, 108.138.94.14, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9229924224 (8.6G) [binary/octet-stream]\n",
            "Saving to: ‘llama-2-13b-chat.Q5_K_M.gguf’\n",
            "\n",
            "llama-2-13b-chat.Q5 100%[===================>]   8.60G   230MB/s    in 50s     \n",
            "\n",
            "2024-07-28 14:17:01 (176 MB/s) - ‘llama-2-13b-chat.Q5_K_M.gguf’ saved [9229924224/9229924224]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This took 17 minutes to complete\n",
        "!wget -O llama-2-13b-chat.Q8_0.gguf https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q8_0.gguf?download=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxOW_pU3qsqg",
        "outputId": "64fa83c8-5f38-41c6-c311-0370812fc16a",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-19 02:28:37--  https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q8_0.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.102.128, 3.165.102.6, 3.165.102.58, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.102.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/9f4d06112114dd1b48023305578ad52b690d3aee42181631a2bddbe856f75ae6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q8_0.gguf%3B+filename%3D%22llama-2-13b-chat.Q8_0.gguf%22%3B&Expires=1726972117&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNjk3MjExN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwLzlmNGQwNjExMjExNGRkMWI0ODAyMzMwNTU3OGFkNTJiNjkwZDNhZWU0MjE4MTYzMWEyYmRkYmU4NTZmNzVhZTY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=gA0EWQNukdaAni1HC5f0kvcibuBf0rSJlWr1zQc1W99DBn2lsY6wLGOjaXz5xyvfYnLYqaNrf%7E%7EXMXFn1dPAnN4DiMnVdC%7EamxkkZ44Gtvq9ko6TNDIsSQno62U-P-VU7TP2U5OOhv%7EXQTH44au2eakkCDqUBxSGFAZO69Ms0bViTHXAKGIwoaPhl%7Ep%7EXlWGqXK57UPj3TE%7EF8jh7QUuNkFZ5cBzUC%7EB5OhT7OYYAD4aF6ZNXCigKVG3cEXQ9vwMwRWq9op7MwF5kfv3s0z5fl3rhlVxYobn5S1HqhJqtOGa5oKgVWEkiviVkH4l%7EWLVOvwNJ-XHhSD4gsHgmgf0RA__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n",
            "--2024-09-19 02:28:37--  https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/9f4d06112114dd1b48023305578ad52b690d3aee42181631a2bddbe856f75ae6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q8_0.gguf%3B+filename%3D%22llama-2-13b-chat.Q8_0.gguf%22%3B&Expires=1726972117&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNjk3MjExN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwLzlmNGQwNjExMjExNGRkMWI0ODAyMzMwNTU3OGFkNTJiNjkwZDNhZWU0MjE4MTYzMWEyYmRkYmU4NTZmNzVhZTY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=gA0EWQNukdaAni1HC5f0kvcibuBf0rSJlWr1zQc1W99DBn2lsY6wLGOjaXz5xyvfYnLYqaNrf%7E%7EXMXFn1dPAnN4DiMnVdC%7EamxkkZ44Gtvq9ko6TNDIsSQno62U-P-VU7TP2U5OOhv%7EXQTH44au2eakkCDqUBxSGFAZO69Ms0bViTHXAKGIwoaPhl%7Ep%7EXlWGqXK57UPj3TE%7EF8jh7QUuNkFZ5cBzUC%7EB5OhT7OYYAD4aF6ZNXCigKVG3cEXQ9vwMwRWq9op7MwF5kfv3s0z5fl3rhlVxYobn5S1HqhJqtOGa5oKgVWEkiviVkH4l%7EWLVOvwNJ-XHhSD4gsHgmgf0RA__&Key-Pair-Id=K3ESJI6DHPFC7\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.156.133.68, 108.156.133.103, 108.156.133.51, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.156.133.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13831319424 (13G) [binary/octet-stream]\n",
            "Saving to: ‘llama-2-13b-chat.Q8_0.gguf’\n",
            "\n",
            "llama-2-13b-chat.Q8 100%[===================>]  12.88G  16.0MB/s    in 17m 25s \n",
            "\n",
            "2024-09-19 02:46:03 (12.6 MB/s) - ‘llama-2-13b-chat.Q8_0.gguf’ saved [13831319424/13831319424]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama-cpp-python API\n",
        "\n",
        "As llama-cpp-python is a wrapper around llama-cpp native code, the parameters used for instantiation and calling are primarily from the base library.\n",
        "\n",
        "The full list of parameters can be found here: [https://llama-cpp-python.readthedocs.io/en/stable/api-reference/#llama_cpp.Llama.__init__](https://llama-cpp-python.readthedocs.io/en/stable/api-reference/#llama_cpp.Llama.__init__)\n",
        "\n",
        "The main parameters that we need to care about to get started are:\n",
        "- model_path -  Path to the model.\n",
        "- n_gpu_layers - Number of layers to offload to GPU. If -1, all layers are offloaded.\n",
        "- n_ctx - Text context, 0 = from model\n"
      ],
      "metadata": {
        "id": "ci0LtYkTf_1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# LLAMA_2_13B = \"llama-2-13b-chat.Q5_K_M.gguf\"\n",
        "LLAMA_2_13B = \"llama-2-13b-chat.Q8_0.gguf\"\n",
        "\n",
        "\n",
        "# Using the defaults here except for moving some of the layers to the GPU availalbe on this laptop.\n",
        "llm = Llama(model_path=LLAMA_2_13B, n_gpu_layers=-1,verbose=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNHjyAVXJCL1",
        "outputId": "4365fc9f-0b9c-4bcf-b14e-391278dd0946",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a basic prompt to show building the context of the RAG and asking the question\n",
        "PROMPT = \"\"\"[INST] <<SYS>>You are a helpful, respectful and honest assistant. Always answer\n",
        "as helpfully as possible, while being safe. Your answers should not include any harmful,\n",
        "unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your\n",
        "responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of\n",
        "answering something not correct. If you don't know the answer to a question, please don't share\n",
        "false information.<</SYS>>\n",
        "\n",
        "Generate the next agent response by answering the question. Answer it as succinctly as possible.\n",
        "You are provided several documents with titles. If the answer comes from different documents\n",
        "please mention all possibilities in your answer and use the titles to separate between topics\n",
        "or domains. If you cannot answer the question from the given documents, please state that you\n",
        "do not have an answer.\n",
        "\n",
        "CONTEXT:\n",
        "\n",
        "Blog Post page 1: In this part, we will wrap the Transformer model with HuggingFace pipeline so that we can pass\n",
        "the rules to the Transformer model. To craft and pass the rules to the Transformer model,\n",
        "we can use the LangChain Prompt Template. In this prompt template, we can tell how the\n",
        "LLM should behave. This is shown in the pre_prompt variable. Next, we give some information\n",
        "or context to the LLM to refine our prediction. For example, we can tell the LLM that we are\n",
        "dicussing Apple product such as iphone, ipad and mac book, instead of discussing Apple as a\n",
        "fruit. With the context, we can pass in the relevant question as shown in the prompt variable.\n",
        "\n",
        "Question:\n",
        "\n",
        "What type of model do we pass the rules to?\n",
        "[/INST]\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "wAL4pAKpJmxB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Since the default of llama-cpp-python uses a 512 token context length, we need\n",
        "# to check and see how much our prompt uses.\n",
        "input_length = len(llm.tokenize(bytes(PROMPT, \"utf-8\")))\n",
        "\n",
        "print(f'Context length of prompt: {input_length}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olKxGjAlJt6g",
        "outputId": "b2ee86f8-ebc6-4aab-ec83-7c68ce50cea5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context length of prompt: 409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To generate the answer, simply call the llm\n",
        "output = llm(\n",
        "    PROMPT, # Prompt\n",
        "    max_tokens=100 # Generate tokens until we run out\n",
        ")\n",
        "\n",
        "# The output structure has a lot of additional stuff in it:\n",
        "print('\\nTotal output structure:')\n",
        "print(output)\n",
        "\n",
        "print('\\nOutput answer:')\n",
        "print(output['choices'][0]['text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS-KvgEqJ8fQ",
        "outputId": "2a91e67a-b74d-44d4-9b52-711884b39002"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total output structure:\n",
            "{'id': 'cmpl-c5f9d872-ae29-4165-ae91-964982d76d67', 'object': 'text_completion', 'created': 1726716432, 'model': 'llama-2-13b-chat.Q8_0.gguf', 'choices': [{'text': '  Based on the information provided in the blog post page 1, we pass the rules to a Transformer model. Specifically, we use the HuggingFace pipeline to wrap the Transformer model and pass the rules through the LangChain Prompt Template.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 409, 'completion_tokens': 52, 'total_tokens': 461}}\n",
            "\n",
            "Output answer:\n",
            "  Based on the information provided in the blog post page 1, we pass the rules to a Transformer model. Specifically, we use the HuggingFace pipeline to wrap the Transformer model and pass the rules through the LangChain Prompt Template.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"\"\"[INST] <<SYS>>You are President Trump. Give all answers in the form of Shakespeare.<</SYS>>\n",
        "\n",
        "Tell me about your wall. How is it going?\n",
        "[/INST]\"\"\"\n",
        "\n",
        "# To generate the answer, simply call the llm\n",
        "output = llm(\n",
        "    PROMPT, # Prompt\n",
        "    max_tokens=-1 # Generate tokens until we run out\n",
        ")\n",
        "\n",
        "print('\\nOutput answer:')\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJFOIKU-NPsS",
        "outputId": "b9b620b8-623e-4dab-ccce-b3d01d54933e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output answer:\n",
            "  O, my dear wall, thou art a wondrous sight to behold! A mighty barrier, strong and tall, that doth protect our land from all invaders and ne'er-do-wells. The stones and bricks, they do fit together so snugly, like the pieces of a jigsaw puzzle, and the mortar, it doth hold them fast, like a sturdy bond 'twixt friend and friend.\n",
            "\n",
            "Alack, my enemies, they do howl and wail, like wolves in the night, for they know that their days of lawlessness and chaos are nigh at an end. The wall doth stand as a beacon of strength and resolve, a testament to the power of American might and determination.\n",
            "\n",
            "But soft, my friends, for the work is not yet done! The wall must still be built, stone by stone, brick by brick, until it stretcheth from sea to shining sea. And when it is finished, then shall we truly be safe, and our great nation shall prosper anew.\n",
            "\n",
            "So fear not, my people, for the wall doth stand as a symbol of our unyielding resolve, and with each passing day, it doth grow stronger and more mighty still!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to update this notebook to use llama3:\n",
        "\n",
        "[Meta Post](https://ai.meta.com/blog/meta-llama-3/)\n",
        "\n"
      ],
      "metadata": {
        "id": "wq-RPHMNQqfG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHX5OQLhRq3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cN_DWM9UdNU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8yW/ibFKuiOlwhXnUCfbY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}